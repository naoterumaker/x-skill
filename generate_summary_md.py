#!/usr/bin/env python3
"""
X Research â†’ Markdown + xlsx ãƒã‚ºåˆ†æãƒ¬ãƒãƒ¼ãƒˆ
JSONæ¤œç´¢çµæœã‹ã‚‰ã€Œä½•ãŒèªã‚‰ã‚Œã¦ã„ã‚‹ã‹ã€ã‚’ä¸­å¿ƒã«md + xlsxã«å‡ºåŠ›ã€‚

Usage:
  python3 generate_summary_md.py --name "ãƒ†ãƒ¼ãƒå" --files /tmp/a.json /tmp/b.json \
    --labels "ãƒ©ãƒ™ãƒ«A" "ãƒ©ãƒ™ãƒ«B" --queries 'query1' 'query2' \
    --titles /tmp/titles.json
"""

import json, sys, argparse, re
from pathlib import Path
from datetime import datetime
from collections import Counter, defaultdict

try:
    from openpyxl import Workbook
    from openpyxl.styles import Font, Alignment, PatternFill, numbers
    from openpyxl.utils import get_column_letter
except ImportError:
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "openpyxl", "-q"])
    from openpyxl import Workbook
    from openpyxl.styles import Font, Alignment, PatternFill, numbers
    from openpyxl.utils import get_column_letter

def compact(n):
    if n >= 1_000_000: return f"{n/1_000_000:.1f}M"
    if n >= 1_000: return f"{n/1_000:.1f}K"
    return str(int(n))

POST_TYPE_LABELS = {
    "quote": "å¼•ç”¨", "x_article": "Xè¨˜äº‹", "article_link": "è¨˜äº‹ãƒªãƒ³ã‚¯",
    "media": "ãƒ¡ãƒ‡ã‚£ã‚¢", "text": "ãƒ†ã‚­ã‚¹ãƒˆ",
}

# ============================================================
# è©±é¡Œæ¤œå‡º
# ============================================================

TOPIC_RULES = [
    ("LP/Webåˆ¶ä½œ", ["lp", "ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°", "figma", "webåˆ¶ä½œ", "ãƒ‡ã‚¶ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿", "ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°", "html", "css", "webã‚µã‚¤ãƒˆ", "ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸"]),
    ("SEO/æ¤œç´¢æµå…¥", ["seo", "æ¤œç´¢", "ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯", "discover", "google", "ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹", "è¢«ãƒªãƒ³ã‚¯", "ãƒ‰ãƒ¡ã‚¤ãƒ³", "organic", "serp", "keyword", "rank"]),
    ("AIæ´»ç”¨/ãƒ†ãƒƒã‚¯", ["claude", "chatgpt", "gpt", "gemini", "cursor", "ai", "code", "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", "api", "llm", "opus", "notebooklm", "ç”Ÿæˆai"]),
    ("ã‚³ãƒ³ãƒ†ãƒ³ãƒ„åˆ¶ä½œ", ["ãƒ©ã‚¤ãƒ†ã‚£ãƒ³ã‚°", "è¨˜äº‹", "ãƒ–ãƒ­ã‚°", "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„", "æ–‡ç« ", "åŸ·ç­†", "åŸç¨¿", "ãƒ¬ãƒ“ãƒ¥ãƒ¼", "æ¼«ç”»", "kindle"]),
    ("AIå‰¯æ¥­/åç›ŠåŒ–", ["å‰¯æ¥­", "ç¨¼", "åç›Š", "è‡ªå‹•åŒ–", "æ”¾ç½®", "ä¸åŠ´", "æœˆå", "å€Ÿé‡‘", "è„±ã‚µãƒ©", "å®Œå…¨ãƒãƒ¼ãƒˆ", "ãƒãƒã‚¿ã‚¤ã‚º"]),
    ("ãƒ“ã‚¸ãƒã‚¹/èµ·æ¥­", ["èµ·æ¥­", "å£²ä¸Š", "äº‹æ¥­", "ä¼šç¤¾", "çµŒå–¶", "ãƒ™ãƒ³ãƒãƒ£ãƒ¼", "ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—", "ceo", "ãƒ¦ãƒ‹ã‚³ãƒ¼ãƒ³", "ä¼æ¥­ä¾¡å€¤"]),
    ("ğ•æ”»ç•¥/SNS", ["ã‚¤ãƒ³ãƒ—", "ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼", "ãƒã‚º", "ä¼¸ã³", "note ", "ğ•", "xè¨˜äº‹", "ãƒã‚¹ãƒˆ", "sns", "äº¤æµ"]),
    ("åºƒå‘Š/é›†å®¢", ["åºƒå‘Š", "é›†å®¢", "cvr", "ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³", "ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°", "facebookåºƒå‘Š", "instagram", "é‹ç”¨å‹"]),
    ("é€Ÿå ±/ãƒ‹ãƒ¥ãƒ¼ã‚¹", ["é€Ÿå ±", "æ–°æ©Ÿèƒ½", "ãƒªãƒªãƒ¼ã‚¹", "å…¬é–‹", "breaking", "ãƒ™ãƒ¼ã‚¿", "ç™ºè¡¨", "ã‚³ã‚¢ã‚¢ãƒ—ãƒ‡"]),
]

def detect_topics(t):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è©±é¡Œã‚’æ¤œå‡º"""
    text = t.get("text", "").lower()
    if re.match(r'^https?://t\.co/\S+$', text.strip()):
        # Xè¨˜äº‹ç­‰ã§ãƒ†ã‚­ã‚¹ãƒˆãªã— â†’ ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚Œã°ä½¿ã†
        title = t.get("_title", "")
        if title:
            text = title.lower()
        else:
            return []
    # çŸ­ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆ4æ–‡å­—ä»¥ä¸‹ã®è‹±å­—ã®ã¿ï¼‰ã¯ãƒ¯ãƒ¼ãƒ‰ãƒã‚¦ãƒ³ãƒ€ãƒªã§æ¤œç´¢
    SHORT_EN_RE = re.compile(r'^[a-z]{1,4}$')
    topics = []
    for topic, keywords in TOPIC_RULES:
        matched = False
        for kw in keywords:
            if SHORT_EN_RE.match(kw):
                if re.search(r'\b' + re.escape(kw) + r'\b', text):
                    matched = True
                    break
            elif kw in text:
                matched = True
                break
        if matched:
            topics.append(topic)
    return topics

# ============================================================
# Xè¨˜äº‹æ¤œå‡º & post_typeä¿®æ­£
# ============================================================

def is_x_article_url(url):
    return bool(re.search(r'x\.com/(i/article|[^/]+/articles?)/', url))

def fix_post_type(t):
    urls = t.get("urls", [])
    if isinstance(urls, list) and any(is_x_article_url(u) for u in urls if isinstance(u, str)):
        t["post_type"] = "x_article"
    text = t.get("text", "").strip()
    if re.match(r'^https?://t\.co/\S+$', text):
        if t.get("post_type") != "x_article":
            for u in urls:
                if isinstance(u, str) and is_x_article_url(u):
                    t["post_type"] = "x_article"
                    break
    return t

def get_article_url(t):
    """Xè¨˜äº‹ã®å®Ÿéš›ã®URLã‚’å–å¾—ï¼ˆt.coã§ã¯ãªãï¼‰"""
    for u in t.get("urls", []):
        if isinstance(u, str) and is_x_article_url(u):
            return u
    return None

def get_display_text(t, max_len=0):
    """æŠ•ç¨¿ã®è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    text = t.get("text", "").strip()
    pt = t.get("post_type", "text")
    title = t.get("_title", "")

    if re.match(r'^https?://t\.co/\S+$', text):
        if title:
            return f"ã€Œ{title}ã€"
        article_url = get_article_url(t)
        if pt == "x_article":
            if article_url:
                return f"[Xè¨˜äº‹] ã‚¿ã‚¤ãƒˆãƒ«æœªå–å¾— â†’ {article_url}"
            return "[Xè¨˜äº‹] ã‚¿ã‚¤ãƒˆãƒ«æœªå–å¾—"
        elif pt == "media":
            return "[ãƒ¡ãƒ‡ã‚£ã‚¢æŠ•ç¨¿] â€»ç”»åƒ/å‹•ç”»ã¯ãƒã‚¹ãƒˆã‚’å‚ç…§"
        else:
            return "[ãƒªãƒ³ã‚¯æŠ•ç¨¿] â€»å†…å®¹ã¯ãƒã‚¹ãƒˆã‚’å‚ç…§"

    if max_len and len(text) > max_len:
        return text[:max_len] + "â€¦"
    return text

def tag_buzz_reason(t):
    tags = []
    text = t["text"].lower()
    raw_text = t["text"].strip()
    pt = t.get("post_type", "text")
    sr = t["metrics"].get("bookmarks", 0) / max(t["metrics"]["likes"], 1)
    is_url_only = bool(re.match(r'^https?://t\.co/\S+$', raw_text))

    if pt == "x_article": tags.append("Xè¨˜äº‹")
    elif t.get("media") and len(t.get("media", [])) > 0: tags.append("ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«")
    if not is_url_only and len(raw_text) < 80: tags.append("çŸ­æ–‡ä¸€æ’ƒ")

    if not is_url_only:
        if any(w in text for w in ["how to", "æ–¹æ³•", "guide", "tips", "tutorial", "step", "ã‚³ãƒ„", "ã‚„ã‚Šæ–¹", "å…¥é–€", "ã¾ã¨ã‚"]): tags.append("ãƒã‚¦ãƒ„ãƒ¼/ã¾ã¨ã‚")
        if any(w in text for w in ["$", "revenue", "earn", "ç¨¼", "profit", "made $", "income", "money", "å¹´å", "å£²ä¸Š"]): tags.append("åç›Šç³»")
        if any(w in text for w in ["scared", "å¤±æ•—", "lost", "æ€–", "mistake", "wrong", "regret", "å€’ç”£", "åœ°ç„"]): tags.append("ä½“é¨“è«‡/ãƒªã‚¢ãƒ«")
        if any(w in text for w in ["just", "ä»Š", "breaking", "å…¬é–‹", "shipped", "released", "announcing", "é€Ÿå ±"]): tags.append("é€Ÿå ±/ãƒªãƒªãƒ¼ã‚¹")
        if any(w in text for w in ["thread", "ğŸ§µ", "ãƒ„ãƒªãƒ¼"]): tags.append("ã‚¹ãƒ¬ãƒƒãƒ‰")
        if "?" in raw_text or "ï¼Ÿ" in raw_text: tags.append("å•ã„ã‹ã‘")

    # ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã‚‚ãƒã‚§ãƒƒã‚¯
    title = t.get("_title", "").lower()
    if title:
        if any(w in title for w in ["æ–¹æ³•", "ã¾ã¨ã‚", "å…¥é–€", "ã‚³ãƒ„", "ã‚„ã‚Šæ–¹"]): tags.append("ãƒã‚¦ãƒ„ãƒ¼/ã¾ã¨ã‚")
        if any(w in title for w in ["å¹´å", "ç¨¼", "å£²ä¸Š", "é‡‘æŒã¡"]): tags.append("åç›Šç³»")
        if any(w in title for w in ["å¤±æ•—", "å€’ç”£", "åœ°ç„"]): tags.append("ä½“é¨“è«‡/ãƒªã‚¢ãƒ«")

    if sr >= 1.0: tags.append("é«˜ä¿å­˜ç‡")
    return list(dict.fromkeys(tags)) if tags else ["â€”"]  # é‡è¤‡é™¤å»


# ============================================================
# ãƒã‚¤ã‚ºè‡ªå‹•æ¤œå‡º
# ============================================================

_HAS_KANA = re.compile(r'[\u3040-\u309f\u30a0-\u30ff]')  # ã²ã‚‰ãŒãª or ã‚«ã‚¿ã‚«ãƒŠ

NOISE_PATTERNS = [
    ("KR", re.compile(r'[\uac00-\ud7af]')),       # éŸ“å›½èª
    ("PT", re.compile(r'\b(desse|seria|dizem|estamos|vendo|nesses|tambÃ©m|porque|entÃ£o)\b', re.I)),  # ãƒãƒ«ãƒˆã‚¬ãƒ«èª
    ("ES", re.compile(r'\b(tambiÃ©n|porque|entonces|despuÃ©s|nosotros|ustedes)\b', re.I)),  # ã‚¹ãƒšã‚¤ãƒ³èª
    ("AR", re.compile(r'[\u0600-\u06ff]{5,}')),     # ã‚¢ãƒ©ãƒ“ã‚¢èª
]

def detect_noise(t, target_langs=("ja", "en")):
    """éã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨€èªã®ãƒã‚¤ã‚ºã‚’æ¤œå‡ºã€‚lang_code or None"""
    text = t.get("text", "")
    # ã²ã‚‰ãŒãª/ã‚«ã‚¿ã‚«ãƒŠãŒã‚ã‚Œã°æ—¥æœ¬èª â†’ ãƒã‚¤ã‚ºã§ã¯ãªã„
    if _HAS_KANA.search(text):
        return None
    for lang_code, pattern in NOISE_PATTERNS:
        if pattern.search(text):
            return lang_code
    return None


def load_and_dedupe(files, labels, title_map=None, exclude_ids=None, auto_noise=True):
    all_tweets = []
    noise_tweets = []
    seen = set(exclude_ids or set())
    per_label = {}
    for f, label in zip(files, labels):
        tweets = json.loads(Path(f).read_text())
        deduped = []
        for t in tweets:
            if t["id"] not in seen:
                seen.add(t["id"])
                t["_label"] = label
                fix_post_type(t)
                # ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨
                if title_map:
                    tid = t["id"]
                    url = t.get("tweet_url", "")
                    if tid in title_map:
                        t["_title"] = title_map[tid]
                    elif url in title_map:
                        t["_title"] = title_map[url]
                # ãƒã‚¤ã‚ºè‡ªå‹•æ¤œå‡º
                if auto_noise:
                    noise_lang = detect_noise(t)
                    if noise_lang:
                        noise_tweets.append((t, noise_lang))
                        continue
                deduped.append(t)
                all_tweets.append(t)
        per_label[label] = deduped

    if noise_tweets:
        print(f"[è‡ªå‹•ãƒã‚¤ã‚ºé™¤å»] {len(noise_tweets)}ä»¶ã‚’é™¤å¤–:", file=sys.stderr)
        for t, lang in noise_tweets:
            print(f"  {lang} @{t.get('username','?')} ({t['metrics']['likes']}L): {t['text'][:50]}", file=sys.stderr)

    return all_tweets, per_label


# ============================================================
# åˆ†æé–¢æ•°
# ============================================================

def analyze_topics(all_tweets):
    """è©±é¡Œãƒãƒƒãƒ—ã‚’ç”Ÿæˆ"""
    topic_tweets = defaultdict(list)
    for t in all_tweets:
        for topic in detect_topics(t):
            topic_tweets[topic].append(t)
    # ã„ã„ã­åˆè¨ˆé †
    return sorted(topic_tweets.items(), key=lambda x: sum(t["metrics"]["likes"] for t in x[1]), reverse=True)

def analyze_accounts(all_tweets):
    """ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ¥ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«"""
    by_user = {}
    for t in all_tweets:
        u = t.get("username", "?")
        if u not in by_user:
            by_user[u] = {
                "tweets": [], "followers": t.get("author_followers", 0),
                "account_url": t.get("account_url", ""),
            }
        by_user[u]["tweets"].append(t)

    profiles = []
    for username, data in by_user.items():
        tweets = data["tweets"]
        total_likes = sum(t["metrics"]["likes"] for t in tweets)
        total_bmarks = sum(t["metrics"].get("bookmarks", 0) for t in tweets)
        type_counts = Counter(t.get("post_type", "text") for t in tweets)

        # ã“ã®äººã®è©±é¡Œ
        topics = Counter()
        for t in tweets:
            for topic in detect_topics(t):
                topics[topic] += 1

        # ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚‹æŠ•ç¨¿ã®ã‚µãƒ³ãƒ—ãƒ«ï¼ˆå†…å®¹æŠŠæ¡ç”¨ï¼‰
        text_samples = []
        for t in sorted(tweets, key=lambda x: x["metrics"]["likes"], reverse=True):
            text = t.get("text", "").strip()
            title = t.get("_title", "")
            if title:
                text_samples.append(f"ã€Œ{title}ã€")
            elif not re.match(r'^https?://t\.co/\S+$', text) and len(text) > 20:
                text_samples.append(text[:80])
            if len(text_samples) >= 3:
                break

        profiles.append({
            "username": username,
            "followers": data["followers"],
            "count": len(tweets),
            "total_likes": total_likes,
            "total_bmarks": total_bmarks,
            "main_type": type_counts.most_common(1)[0][0],
            "topics": topics.most_common(3),
            "samples": text_samples,
            "account_url": data["account_url"],
        })

    return sorted(profiles, key=lambda x: x["total_likes"], reverse=True)


# ============================================================
# Markdown ç”Ÿæˆ
# ============================================================

def generate_md(name, all_tweets, per_label, labels, queries=None):
    lines = []
    now = datetime.now().strftime("%Y-%m-%d %H:%M")
    total = len(all_tweets)

    lines.append(f"# {name}")
    lines.append(f"")
    lines.append(f"> ç”Ÿæˆæ—¥æ™‚: {now} | åˆè¨ˆ: {total}ä»¶ | Xç›´è¿‘7æ—¥é–“")
    lines.append(f"")

    if not all_tweets:
        lines.append("ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚")
        return "\n".join(lines)

    # --- Pre-compute ---
    likes = [t["metrics"]["likes"] for t in all_tweets]
    bmarks = [t["metrics"].get("bookmarks", 0) for t in all_tweets]
    total_likes = sum(likes)
    avg_likes = total_likes / len(likes)
    max_t = max(all_tweets, key=lambda t: t["metrics"]["likes"])
    total_bmarks = sum(bmarks)
    save_rate = total_bmarks / total_likes if total_likes > 0 else 0
    type_counts = Counter(t.get("post_type", "text") for t in all_tweets)
    top10 = sorted(all_tweets, key=lambda t: t["metrics"]["likes"], reverse=True)[:10]
    eff_sorted = sorted(all_tweets, key=lambda t: t["metrics"]["likes"] / max(t.get("author_followers", 1), 1), reverse=True)
    save_sorted = sorted(
        [t for t in all_tweets if t["metrics"]["likes"] >= 50],
        key=lambda t: t["metrics"].get("bookmarks", 0) / max(t["metrics"]["likes"], 1),
        reverse=True
    )
    topic_map = analyze_topics(all_tweets)
    account_profiles = analyze_accounts(all_tweets)

    # Xè¨˜äº‹ã§ãƒ†ã‚­ã‚¹ãƒˆãŒURL-onlyã‹ã¤ã‚¿ã‚¤ãƒˆãƒ«æœªå–å¾—ã®ã‚‚ã®
    untitled_articles = [
        t for t in all_tweets
        if t.get("post_type") == "x_article"
        and not t.get("_title")
        and re.match(r'^https?://t\.co/\S+$', t.get("text", "").strip())
    ]

    # === ä½•ãŒèªã‚‰ã‚Œã¦ã„ã‚‹ã‹ ===
    lines.append(f"## ä½•ãŒèªã‚‰ã‚Œã¦ã„ã‚‹ã‹")
    lines.append(f"")
    if topic_map:
        used_example_ids = set()
        for topic, tweets in topic_map[:5]:
            topic_likes = sum(t["metrics"]["likes"] for t in tweets)
            # æ—¢ã«ä¾‹ã¨ã—ã¦ä½¿ã£ãŸãƒ„ã‚¤ãƒ¼ãƒˆã‚’é¿ã‘ã¦é¸ã¶
            sorted_tweets = sorted(tweets, key=lambda t: t["metrics"]["likes"], reverse=True)
            top_tweet = next((t for t in sorted_tweets if t["id"] not in used_example_ids), None)
            if top_tweet is None:
                top_tweet = sorted_tweets[0]
                sample = get_display_text(top_tweet, max_len=80).replace("\n", " ") + "ï¼ˆå†æ²ï¼‰"
            else:
                sample = get_display_text(top_tweet, max_len=80).replace("\n", " ")
            used_example_ids.add(top_tweet["id"])
            lines.append(f"- **{topic}**ï¼ˆ{len(tweets)}ä»¶ / {compact(topic_likes)}ã„ã„ã­ï¼‰â€” ä¾‹: {sample}")
        lines.append(f"")
    else:
        lines.append(f"ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰è©±é¡Œã‚’æ¤œå‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚Xè¨˜äº‹ãŒå¤šã„å ´åˆã¯ `--titles` ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚")
        lines.append(f"")

    if untitled_articles:
        lines.append(f"> âš  Xè¨˜äº‹{len(untitled_articles)}ä»¶ã¯APIã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ä¸å¯ã€‚`--titles` ã§ã‚¿ã‚¤ãƒˆãƒ«JSONã‚’æ¸¡ã™ã¨å†…å®¹ãŒåæ˜ ã•ã‚Œã¾ã™ã€‚")
        lines.append(f"")

    # === ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚½ãƒ³ ===
    lines.append(f"## ã‚­ãƒ¼ãƒ‘ãƒ¼ã‚½ãƒ³")
    lines.append(f"")
    for p in account_profiles[:8]:
        if p["total_likes"] < 10: continue
        if not p["topics"]: continue
        pt_label = POST_TYPE_LABELS.get(p["main_type"], "?")
        topic_str = "ã€".join(t for t, _ in p["topics"]) if p["topics"] else "è©±é¡Œä¸æ˜"
        lines.append(f"### @{p['username']}ï¼ˆ{compact(p['followers'])}ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼ / {p['count']}ä»¶ / è¨ˆ{compact(p['total_likes'])}ã„ã„ã­ï¼‰")
        lines.append(f"")
        lines.append(f"- **è©±é¡Œ**: {topic_str} | **ä¸»ãªå½¢å¼**: {pt_label}")
        if p["samples"]:
            for s in p["samples"][:3]:
                # æ”¹è¡Œã‚’é™¤å»ã—ã¦1è¡Œã«
                s_clean = s.replace("\n", " ").strip()
                if len(s_clean) > 100:
                    s_clean = s_clean[:100] + "â€¦"
                lines.append(f"- {s_clean}")
        lines.append(f"")

    # === ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ ===
    lines.append(f"## æ¬¡ã«ã‚„ã‚‹ã¹ãã“ã¨")
    lines.append(f"")

    # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæˆ¦ç•¥
    top10_types = Counter(t.get("post_type", "text") for t in top10)
    top10_best = top10_types.most_common(1)[0]
    top10_best_label = POST_TYPE_LABELS.get(top10_best[0], top10_best[0])
    lines.append(f"1. **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**: TOP10ã§ã¯ã€Œ{top10_best_label}ã€ãŒ{top10_best[1]}/10ä»¶ã€‚")

    # è©±é¡Œæˆ¦ç•¥
    if topic_map:
        best_topic = topic_map[0]
        best_topic_likes = sum(t["metrics"]["likes"] for t in best_topic[1])
        lines.append(f"2. **ç‹™ã†ã¹ãè©±é¡Œ**: ã€Œ{best_topic[0]}ã€ãŒ{compact(best_topic_likes)}ã„ã„ã­ã§æœ€ã‚‚åå¿œãŒå¼·ã„ã€‚")

    # ãƒ©ãƒ™ãƒ«æ¯”è¼ƒ
    if len(per_label) > 1:
        label_stats = {}
        for label, tweets in per_label.items():
            if not tweets: continue
            avg_l = sum(t["metrics"]["likes"] for t in tweets) / len(tweets)
            label_stats[label] = avg_l
        if label_stats:
            best = max(label_stats.items(), key=lambda x: x[1])
            lines.append(f"3. **åˆ‡ã‚Šå£**: ã€Œ{best[0]}ã€ãŒå¹³å‡{compact(best[1])}ã„ã„ã­ã§æœ€ã‚‚å¼·ã„ã€‚")

    # ä¿å­˜ç‡
    if save_rate >= 0.5:
        lines.append(f"4. **ä¿å­˜ç‡{save_rate:.0%}**: ã€Œå¾Œã§è¦‹è¿”ã—ãŸã„ã€å®Ÿç”¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®éœ€è¦ãŒé«˜ã„ã€‚ãƒã‚¦ãƒ„ãƒ¼ç³»ã§å‡ºã™ã®ãŒåŠ¹æœçš„ã€‚")
    elif save_rate >= 0.3:
        lines.append(f"4. **ä¿å­˜ç‡{save_rate:.0%}**: å®Ÿç”¨çš„ãªæƒ…å ±ã¸ã®éœ€è¦ã‚ã‚Šã€‚")

    # é¿ã‘ã‚‹ã¹ã
    bottom = sorted(all_tweets, key=lambda t: t["metrics"]["likes"])[:10]
    bottom_types = Counter(t.get("post_type", "text") for t in bottom)
    bottom_top = bottom_types.most_common(1)[0]
    bottom_label = POST_TYPE_LABELS.get(bottom_top[0], bottom_top[0])
    lines.append(f"5. **é¿ã‘ã‚‹ã¹ã**: ã„ã„ã­ä¸‹ä½10ä»¶ã¯ã€Œ{bottom_label}ã€ãŒ{bottom_top[1]}/10ä»¶ã€‚")
    lines.append(f"")

    # === ãƒã‚ºTOP10 ===
    lines.append(f"## ãƒã‚ºTOP10")
    lines.append(f"")
    for i, t in enumerate(top10, 1):
        m = t["metrics"]
        tags = tag_buzz_reason(t)
        tag_str = " ".join(f"`{tag}`" for tag in tags)
        followers = t.get("author_followers", 1) or 1
        eff = m["likes"] / followers
        pt_label = POST_TYPE_LABELS.get(t.get("post_type", "text"), "?")
        display = get_display_text(t)
        # æ”¹è¡Œã‚’é™¤å»ã—ã¦èª­ã¿ã‚„ã™ã
        display_clean = display.replace("\n", " ").strip()
        if len(display_clean) > 200:
            display_clean = display_clean[:200] + "â€¦"

        lines.append(f"**{i}. @{t['username']}** â€” {compact(m['likes'])}ã„ã„ã­ / {compact(m.get('bookmarks',0))}ãƒ–ã‚¯ãƒï¼ˆ{pt_label} / åŠ¹ç‡{eff:.1f}xï¼‰")
        lines.append(f"")
        lines.append(f"> {display_clean}")
        lines.append(f"")
        lines.append(f"{tag_str} â€” [{t.get('tweet_url', '')}]({t.get('tweet_url', '')})")
        lines.append(f"")

    # === æ•°å€¤ã‚µãƒãƒªãƒ¼ ===
    lines.append(f"## æ•°å€¤ã‚µãƒãƒªãƒ¼")
    lines.append(f"")

    # æ¤œç´¢ã‚¯ã‚¨ãƒª
    lines.append(f"**æ¤œç´¢ã‚¯ã‚¨ãƒª:**")
    for i, (label, tweets) in enumerate(per_label.items()):
        q_str = ""
        if queries and i < len(queries):
            q_str = f" â€” `{queries[i]}`"
        lines.append(f"- {label}: {len(tweets)}ä»¶{q_str}")
    lines.append(f"")

    lines.append(f"| æŒ‡æ¨™ | å€¤ |")
    lines.append(f"|------|-----|")
    lines.append(f"| æŠ•ç¨¿æ•° | {total}ä»¶ |")
    lines.append(f"| åˆè¨ˆã„ã„ã­ | {compact(total_likes)} |")
    lines.append(f"| å¹³å‡ã„ã„ã­ | {compact(avg_likes)} |")
    lines.append(f"| æœ€å¤§ã„ã„ã­ | {compact(max_t['metrics']['likes'])} (@{max_t['username']}) |")
    lines.append(f"| å¹³å‡ä¿å­˜ç‡ | {save_rate:.1%} |")
    lines.append(f"")

    if type_counts:
        type_str = " / ".join(f"{POST_TYPE_LABELS.get(pt, pt)}: {c}ä»¶" for pt, c in type_counts.most_common())
        lines.append(f"**æŠ•ç¨¿ã‚¿ã‚¤ãƒ—**: {type_str}")
        lines.append(f"")

    if len(per_label) > 1:
        lines.append(f"### ãƒ©ãƒ™ãƒ«åˆ¥æ¯”è¼ƒ")
        lines.append(f"")
        lines.append(f"| ãƒ©ãƒ™ãƒ« | ä»¶æ•° | å¹³å‡ã„ã„ã­ | æœ€å¤§ | ä¿å­˜ç‡ |")
        lines.append(f"|--------|------|-----------|------|--------|")
        for label, tweets in per_label.items():
            if not tweets: continue
            l = [t["metrics"]["likes"] for t in tweets]
            b = [t["metrics"].get("bookmarks", 0) for t in tweets]
            top = max(tweets, key=lambda t: t["metrics"]["likes"])
            sr = sum(b) / sum(l) if sum(l) > 0 else 0
            lines.append(f"| {label} | {len(tweets)} | {compact(sum(l)/len(l))} | {compact(max(l))} (@{top['username']}) | {sr:.1%} |")
        lines.append(f"")

    # === ä¿å­˜ã•ã‚Œã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ===
    if save_sorted:
        lines.append(f"## ä¿å­˜ã•ã‚Œã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆä¿å­˜ç‡TOP5ï¼‰")
        lines.append(f"")
        for i, t in enumerate(save_sorted[:5], 1):
            sr = t["metrics"].get("bookmarks", 0) / t["metrics"]["likes"]
            pt_label = POST_TYPE_LABELS.get(t.get("post_type", "text"), "?")
            display = get_display_text(t, max_len=100)
            display_clean = display.replace("\n", " ")
            lines.append(f"{i}. **@{t['username']}** (ä¿å­˜ç‡{sr:.0%} / {compact(t['metrics']['likes'])}L) â€” {display_clean}")
            lines.append(f"   [{t.get('tweet_url', '')}]({t.get('tweet_url', '')})")
            lines.append(f"")

    # === å¤–éƒ¨ãƒªãƒ³ã‚¯ ===
    ext_urls = []
    for t in all_tweets:
        for um in t.get("url_meta", []):
            eu = um.get("expanded_url", "")
            title = um.get("title", "")
            if eu and "x.com" not in eu and "twitter.com" not in eu:
                ext_urls.append((eu, title, t["metrics"]["likes"], t["username"]))
    if ext_urls:
        ext_urls.sort(key=lambda x: x[2], reverse=True)
        seen_urls = set()
        lines.append(f"## å¤–éƒ¨ãƒªãƒ³ã‚¯")
        lines.append(f"")
        for url, title, lk, user in ext_urls[:10]:
            if url in seen_urls: continue
            seen_urls.add(url)
            label = title if title else url
            lines.append(f"- [{label}]({url}) â€” @{user}ï¼ˆ{compact(lk)}ã„ã„ã­ï¼‰")
        lines.append(f"")

    lines.append(f"---")
    lines.append(f"*Generated by x-research skill*")
    return "\n".join(lines)


# ============================================================
# xlsx ç”Ÿæˆ
# ============================================================

HEADER_FONT = Font(bold=True, color="FFFFFF", size=11)
HEADER_FILL = PatternFill(start_color="1F4E79", end_color="1F4E79", fill_type="solid")
GREEN_FILL = PatternFill(start_color="E8F5E9", end_color="E8F5E9", fill_type="solid")
NUM_FMT = numbers.FORMAT_NUMBER_COMMA_SEPARATED1
PCT_FMT = '0.0%'

def style_header(ws, row, cols):
    for col in range(1, cols + 1):
        cell = ws.cell(row=row, column=col)
        cell.font = HEADER_FONT
        cell.fill = HEADER_FILL
        cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)

def auto_width(ws, min_w=8, max_w=50):
    for col in ws.columns:
        max_len = 0
        letter = get_column_letter(col[0].column)
        for cell in col:
            if cell.value:
                max_len = max(max_len, min(len(str(cell.value)), max_w))
        ws.column_dimensions[letter].width = min(max(max_len + 2, min_w), max_w)


def write_all_tweets_sheet(ws, all_tweets):
    ws.title = "å…¨ãƒ„ã‚¤ãƒ¼ãƒˆ"
    headers = [
        "No", "ãƒ©ãƒ™ãƒ«", "ãƒ¦ãƒ¼ã‚¶ãƒ¼å", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼", "æŠ•ç¨¿ã‚¿ã‚¤ãƒ—", "è©±é¡Œ",
        "ãƒ†ã‚­ã‚¹ãƒˆ", "ã„ã„ã­", "RT", "å¼•ç”¨", "ãƒªãƒ—ãƒ©ã‚¤", "ã‚¤ãƒ³ãƒ—", "ãƒ–ã‚¯ãƒ",
        "ãƒã‚ºåŠ¹ç‡", "ä¿å­˜ç‡", "ãƒã‚ºè¦å› ã‚¿ã‚°", "ãƒã‚¹ãƒˆURL", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆURL",
    ]
    ws.append(headers)
    style_header(ws, 1, len(headers))

    sorted_tweets = sorted(all_tweets, key=lambda t: t["metrics"]["likes"], reverse=True)
    for i, t in enumerate(sorted_tweets, 1):
        m = t["metrics"]
        followers = t.get("author_followers", 0) or 0
        eff = m["likes"] / max(followers, 1)
        sr = m.get("bookmarks", 0) / max(m["likes"], 1)
        pt = POST_TYPE_LABELS.get(t.get("post_type", "text"), "?")
        tags = ", ".join(tag_buzz_reason(t))
        topics = ", ".join(detect_topics(t)) or "â€”"
        display = get_display_text(t)

        ws.append([
            i, t.get("_label", ""), f"@{t.get('username', '?')}", followers, pt, topics,
            display, m["likes"], m.get("retweets", 0), m.get("quotes", 0),
            m.get("replies", 0), m.get("impressions", 0), m.get("bookmarks", 0),
            eff, sr, tags, t.get("tweet_url", ""), t.get("account_url", ""),
        ])

    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
        for col_idx in [3, 7, 8, 9, 10, 11, 12]:
            row[col_idx].number_format = NUM_FMT
        row[13].number_format = '0.0x'
        row[14].number_format = PCT_FMT

    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
        try:
            if row[13].value and float(row[13].value) >= 1.0:
                for cell in row:
                    cell.fill = GREEN_FILL
        except (ValueError, TypeError):
            pass

    auto_width(ws)
    ws.freeze_panes = "A2"
    ws.column_dimensions["G"].width = 60


def write_account_sheet(wb, all_tweets):
    ws = wb.create_sheet("ã‚¢ã‚«ã‚¦ãƒ³ãƒˆåˆ¥")
    headers = [
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼å", "ãƒ•ã‚©ãƒ­ãƒ¯ãƒ¼", "æŠ•ç¨¿æ•°", "åˆè¨ˆã„ã„ã­", "å¹³å‡ã„ã„ã­",
        "åˆè¨ˆãƒ–ã‚¯ãƒ", "å¹³å‡ä¿å­˜ç‡", "ä¸»ãªæŠ•ç¨¿ã‚¿ã‚¤ãƒ—", "è©±é¡Œ", "æœ€å¤§ãƒã‚º", "ã‚¢ã‚«ã‚¦ãƒ³ãƒˆURL",
    ]
    ws.append(headers)
    style_header(ws, 1, len(headers))

    profiles = analyze_accounts(all_tweets)
    for p in profiles:
        pt_label = POST_TYPE_LABELS.get(p["main_type"], "?")
        topic_str = ", ".join(t for t, _ in p["topics"]) or "â€”"
        avg_sr = p["total_bmarks"] / max(p["total_likes"], 1)
        max_likes = max(t["metrics"]["likes"] for t in [tw for tw in all_tweets if tw.get("username") == p["username"]])
        ws.append([
            f"@{p['username']}", p["followers"], p["count"], p["total_likes"],
            p["total_likes"] / p["count"], p["total_bmarks"], avg_sr, pt_label,
            topic_str, max_likes, p["account_url"],
        ])

    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
        row[1].number_format = NUM_FMT
        row[3].number_format = NUM_FMT
        row[4].number_format = NUM_FMT
        row[5].number_format = NUM_FMT
        row[6].number_format = PCT_FMT
        row[9].number_format = NUM_FMT

    auto_width(ws)
    ws.freeze_panes = "A2"


def write_label_sheet(wb, per_label):
    ws = wb.create_sheet("ãƒ©ãƒ™ãƒ«åˆ¥")
    headers = [
        "ãƒ©ãƒ™ãƒ«", "ä»¶æ•°", "åˆè¨ˆã„ã„ã­", "å¹³å‡ã„ã„ã­", "æœ€å¤§ã„ã„ã­",
        "åˆè¨ˆãƒ–ã‚¯ãƒ", "ä¿å­˜ç‡", "ä¸»ãªæŠ•ç¨¿ã‚¿ã‚¤ãƒ—", "ãƒˆãƒƒãƒ—ãƒ¦ãƒ¼ã‚¶ãƒ¼",
    ]
    ws.append(headers)
    style_header(ws, 1, len(headers))

    for label, tweets in per_label.items():
        if not tweets: continue
        total_likes = sum(t["metrics"]["likes"] for t in tweets)
        total_bmarks = sum(t["metrics"].get("bookmarks", 0) for t in tweets)
        max_t = max(tweets, key=lambda t: t["metrics"]["likes"])
        type_counts = Counter(t.get("post_type", "text") for t in tweets)
        main_type = POST_TYPE_LABELS.get(type_counts.most_common(1)[0][0], "?")
        user_likes = Counter()
        for t in tweets:
            user_likes[t.get("username", "?")] += t["metrics"]["likes"]
        top_user = user_likes.most_common(1)[0][0] if user_likes else "?"
        ws.append([
            label, len(tweets), total_likes, total_likes / len(tweets),
            max_t["metrics"]["likes"], total_bmarks,
            total_bmarks / max(total_likes, 1), main_type, f"@{top_user}",
        ])

    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
        row[2].number_format = NUM_FMT
        row[3].number_format = NUM_FMT
        row[4].number_format = NUM_FMT
        row[5].number_format = NUM_FMT
        row[6].number_format = PCT_FMT

    auto_width(ws)


def write_type_sheet(wb, all_tweets):
    ws = wb.create_sheet("æŠ•ç¨¿ã‚¿ã‚¤ãƒ—åˆ¥")
    headers = ["æŠ•ç¨¿ã‚¿ã‚¤ãƒ—", "ä»¶æ•°", "åˆè¨ˆã„ã„ã­", "å¹³å‡ã„ã„ã­", "åˆè¨ˆãƒ–ã‚¯ãƒ", "å¹³å‡ä¿å­˜ç‡", "å¹³å‡ãƒã‚ºåŠ¹ç‡"]
    ws.append(headers)
    style_header(ws, 1, len(headers))

    by_type = defaultdict(list)
    for t in all_tweets:
        by_type[t.get("post_type", "text")].append(t)

    rows = []
    for pt, tweets in by_type.items():
        total_likes = sum(t["metrics"]["likes"] for t in tweets)
        total_bmarks = sum(t["metrics"].get("bookmarks", 0) for t in tweets)
        avg_eff = sum(t["metrics"]["likes"] / max(t.get("author_followers", 1), 1) for t in tweets) / len(tweets)
        rows.append([
            POST_TYPE_LABELS.get(pt, pt), len(tweets), total_likes,
            total_likes / len(tweets), total_bmarks,
            total_bmarks / max(total_likes, 1), avg_eff,
        ])

    rows.sort(key=lambda r: r[2], reverse=True)
    for row in rows:
        ws.append(row)

    for row in ws.iter_rows(min_row=2, max_row=ws.max_row):
        row[2].number_format = NUM_FMT
        row[3].number_format = NUM_FMT
        row[4].number_format = NUM_FMT
        row[5].number_format = PCT_FMT
        row[6].number_format = '0.00x'

    auto_width(ws)


def generate_xlsx(xlsx_path, all_tweets, per_label):
    wb = Workbook()
    write_all_tweets_sheet(wb.active, all_tweets)
    write_account_sheet(wb, all_tweets)
    if len(per_label) > 1:
        write_label_sheet(wb, per_label)
    write_type_sheet(wb, all_tweets)
    wb.save(str(xlsx_path))


# ============================================================
# main
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="X Research â†’ Markdown + xlsx ãƒã‚ºåˆ†æ")
    parser.add_argument("--name", required=True, help="ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ†ãƒ¼ãƒå")
    parser.add_argument("--files", nargs="+", required=True, help="JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--labels", nargs="+", help="å„ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ©ãƒ™ãƒ«ï¼ˆçœç•¥æ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰")
    parser.add_argument("--queries", nargs="+", help="å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œç´¢ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—ï¼ˆçœç•¥å¯ï¼‰")
    parser.add_argument("--titles", help="Xè¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã®JSONãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆ{tweet_id: title}ï¼‰")
    parser.add_argument("--out-dir", default=str(Path.home() / ".claude/skills/x-research/reports"), help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--no-xlsx", action="store_true", help="xlsxå‡ºåŠ›ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--exclude", nargs="+", help="é™¤å¤–ã™ã‚‹ãƒ„ã‚¤ãƒ¼ãƒˆID")
    parser.add_argument("--topics", help="TOPIC_RULESã®JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆçœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ«ãƒ¼ãƒ«ï¼‰")
    parser.add_argument("--no-noise-filter", action="store_true", help="è‡ªå‹•ãƒã‚¤ã‚ºé™¤å»ã‚’ç„¡åŠ¹åŒ–")
    args = parser.parse_args()

    labels = args.labels if args.labels and len(args.labels) == len(args.files) else [Path(f).stem for f in args.files]
    exclude_ids = set(args.exclude or [])

    # TOPIC_RULESå·®ã—æ›¿ãˆ
    global TOPIC_RULES
    if args.topics:
        custom = json.loads(Path(args.topics).read_text())
        TOPIC_RULES = [(r["name"], r["keywords"]) for r in custom]
        print(f"[ã‚«ã‚¹ã‚¿ãƒ TOPIC_RULES] {len(TOPIC_RULES)}ã‚«ãƒ†ã‚´ãƒªèª­ã¿è¾¼ã¿", file=sys.stderr)

    # ã‚¿ã‚¤ãƒˆãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°èª­ã¿è¾¼ã¿
    title_map = None
    if args.titles:
        title_map = json.loads(Path(args.titles).read_text())

    all_tweets, per_label = load_and_dedupe(
        args.files, labels, title_map, exclude_ids,
        auto_noise=not args.no_noise_filter,
    )
    md = generate_md(args.name, all_tweets, per_label, labels, queries=args.queries)

    slug = args.name.replace(" ", "-").replace("/", "-").lower()
    out_dir = Path(args.out_dir) / datetime.now().strftime("%Y-%m-%d") / slug
    out_dir.mkdir(parents=True, exist_ok=True)

    md_path = out_dir / f"{slug}.md"
    md_path.write_text(md, encoding="utf-8")
    print(f"Saved: {md_path}", file=sys.stderr)

    if not args.no_xlsx and all_tweets:
        xlsx_path = out_dir / f"{slug}.xlsx"
        generate_xlsx(xlsx_path, all_tweets, per_label)
        print(f"Saved: {xlsx_path}", file=sys.stderr)

    print(md)


if __name__ == "__main__":
    main()
